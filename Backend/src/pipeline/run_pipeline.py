from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Request
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from src.extraction.extract_pipeline import _extract_text_from_pdf
from src.summerizer.llm_client import generate_summary
from src.storage.gcs_client import get_gcs_client
# from src.anomaly_detector.ano_detector_agent import anomaly_detection_pipeline
from src.compliance_checker.compliance_agent import ComplianceAgent

from src.Email_Sender.notification_queue import notification_queue
from src.Email_Sender.risk_eval import is_high_risk
from src.Email_Sender.notification_worker import notification_worker

from src.analytics.analytics_api import router as analytics_router
from src.analytics.analytics_api import router as analytics_router
import traceback
import re
import json
import numpy as np
import requests
import io
import tempfile
import subprocess
from PIL import Image, ImageDraw, ImageFont
from fastapi.encoders import jsonable_encoder
from dotenv import load_dotenv
from typing import Optional, List, Dict, Any
import logging
from datetime import datetime, timedelta
import os
import uuid

load_dotenv()

# Configure logging with more detailed format
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def clean_json_string(json_str: str) -> str:
    """Clean JSON string by removing invalid control characters and fixing common issues"""
    # Remove markdown code blocks
    json_str = json_str.replace("```json", "").replace("```", "").strip()
    
    
    # Remove or replace control characters (except \n, \r, \t which are valid in JSON strings)
    import string
    valid_chars = string.printable + '\n\r\t'
    cleaned = ''.join(char if char in valid_chars else ' ' for char in json_str)
    
    # Fix common JSON issues
    cleaned = re.sub(r',\s*([}\]])', r'\1', cleaned)
    
    # Remove any non-printable characters that might remain
    cleaned = ''.join(char if ord(char) >= 32 or char in '\n\r\t' else ' ' for char in cleaned)
    
    return cleaned.strip()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Handle application startup and shutdown events"""
    logger.info("[START] Starting SEBI Compliance Backend Server")
    print("[APP] FastAPI application starting...")

    # Startup tasks
    try:
        # Verify GCS configuration
        gcs_client = get_gcs_client()
        logger.info(f"[OK] GCS client initialized with bucket: {gcs_client.bucket_name}")
        yield
    except Exception as e:
        logger.error(f"[ERROR] Startup error: {e}\n{traceback.format_exc()}")
        raise
    finally:
        # Shutdown tasks
        logger.info("[SHUTDOWN] Application shutting down...")
        print("[STOP] FastAPI application shutting down...")
        try:
            logger.info("[OK] Cleanup completed")
        except Exception as e:
            logger.error(f"[ERROR] Cleanup error: {e}")
    
app = FastAPI(
    title="SEBI Compliance API",
    description="FastAPI backend for SEBI compliance document analysis",
    version="1.0.0",
    lifespan=lifespan,
    timeout=600,  # 10 minutes timeout
)

# Add CORS middleware for frontend integration
environment = os.getenv("ENVIRONMENT", "production")
frontend_url = os.getenv("FRONTEND_URL", "")

cors_origins = [
    # Local development
    "http://localhost:3000",
    "http://localhost:3001",
    "http://localhost:3002",
    "http://127.0.0.1:3000",
    "http://127.0.0.1:3001",
    # Production frontend (Cloud Run) - all known variants
    "https://frontend-service-127310351608.us-central1.run.app",
    "https://reglex-frontend-127310351608.us-central1.run.app",
    "https://reglex-frontend-305534435339.us-central1.run.app",
    "https://reglex-frontend-ifmliui2lq-uc.a.run.app",
    "https://reglex-frontend.vercel.app",
    "https://sebi-compliance-frontend.vercel.app",
    "https://sebi-compliance-backend.vercel.app",
]

# Add custom frontend URL if provided and not already in list
if frontend_url and frontend_url != "*" and frontend_url not in cors_origins:
    cors_origins.append(frontend_url)

# Decision: For production, we want specific origins if we need credentials.
# For simplicity/hackathon, we'll allow all methods/headers for these origins.
if frontend_url == "*" or environment == "development":
    logger.info("[CORS] Allowing all origins (allow_credentials=False)")
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=False,
        allow_methods=["*"],
        allow_headers=["*"],
        expose_headers=["*"],
    )
else:
    logger.info(f"[CORS] Allowing specific origins: {cors_origins}")
    app.add_middleware(
        CORSMiddleware,
        allow_origins=cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
        expose_headers=["*"],
    )

logger.info(f"[CORS] Final configuration complete")

# Include analytics router
app.include_router(analytics_router)
logger.info("[APP] Analytics router registered")

@app.get("/")
async def root():
    """Root endpoint with API information"""
    logger.info("[API] Root endpoint accessed")
    return {
        "message": "SEBI Compliance API is running",
        "status": "healthy",
        "version": "1.0.0",
        "endpoints": [
            {"path": "/", "method": "GET", "description": "API information"},
            {"path": "/health", "method": "GET", "description": "Health check"},
            {"path": "/upload-pdf/", "method": "POST", "description": "Upload PDF for analysis"}
        ]
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    logger.info("[API] Health check endpoint accessed")
    try:
        gcs_client = get_gcs_client()
        bucket_name = gcs_client.bucket_name
        return {
            "status": "healthy",
            "message": f"SEBI Compliance Backend is operational. GCS bucket: {bucket_name}",
            "timestamp": datetime.now().isoformat(),
            "version": "1.0.0"
        }
    except Exception as e:
        logger.error(f"[HEALTH] Failed to initialize GCS client: {e}")
        return {
            "status": "unhealthy",
            "message": f"GCS client error: {str(e)}",
            "timestamp": datetime.now().isoformat(),
            "version": "1.0.0"
        }

@app.post("/api/auth/login")
async def login(request: Request):
    """Demo login endpoint for authentication"""
    logger.info("[AUTH] Login endpoint accessed")
    try:
        # Parse request body
        body = await request.json()
        email = body.get("email", "")
        password = body.get("password", "")
        
        # Demo credentials check
        if email == "Test-01@gmail.com" and password == "12345678":
            # Generate a demo token
            token = "dummy-auth-token-for-demo-purposes"
            logger.info(f"[AUTH] Login successful for {email}")
            return JSONResponse(
                status_code=200,
                content={
                    "success": True,
                    "message": "Login successful",
                    "token": token,
                    "user": {
                        "email": email,
                        "name": "Demo User"
                    }
                }
            )
        else:
            logger.warning(f"[AUTH] Login failed for {email}")
            return JSONResponse(
                status_code=401,
                content={
                    "success": False,
                    "message": "Invalid email or password. Use Test-01@gmail.com / 12345678"
                }
            )
    except Exception as e:
        logger.error(f"[AUTH] Login error: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"An error occurred during login: {str(e)}"
            }
        )

@app.options("/api/auth/login")
async def login_options():
    """Handle OPTIONS preflight request for login endpoint"""
    return JSONResponse(
        status_code=200,
        content={},
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "POST, OPTIONS",
            "Access-Control-Allow-Headers": "*",
            "Access-Control-Max-Age": "3600"
        }
    )

@app.post("/upload-pdf/")
async def run_backend(file: UploadFile = File(...), lang: Optional[str] = Form(None)):
    document_id = f"doc_{uuid.uuid4().hex[:12]}_{int(datetime.now().timestamp())}"
    
    logger.info(f"[UPLOAD] Upload request received: file={file.filename}, size={file.size if hasattr(file, 'size') else 'unknown'}, lang={lang}, doc_id={document_id}")

    if lang is None:
        lang = "English"
        logger.info(f"[LANG] Using default language: {lang}")
        
    gcs_client = get_gcs_client()
    
    try:
        content = await file.read()
        upload_metadata = {
            "document_id": document_id,
            "filename": file.filename,
            "file_size": len(content),
            "content_type": file.content_type,
            "language": lang,
            "uploaded_at": datetime.now().isoformat(),
            "processing_status": "started"
        }
        
        logger.info(f"[GCS] Storing metadata for document {document_id}")
        gcs_client.upload_document_metadata(document_id, upload_metadata)
        
        logger.info(f"[GCS] Storing original file for document {document_id}")
        gcs_client.upload_document_file(document_id, content, file.filename)
        
        logger.info(f"[EXTRACT] Extracting text from PDF ({len(content)} bytes)")
        try:
            text = _extract_text_from_pdf(content)
        except ValueError as e:
            logger.error(f"[EXTRACT] Text extraction error: {e}")
            raise HTTPException(status_code=400, detail=str(e))
        except Exception as e:
            logger.error(f"[EXTRACT] Unexpected extraction error: {e}")
            raise HTTPException(status_code=500, detail="Unexpected error during PDF text extraction")
        
        # Double-check text extraction was successful
        if not text or text.strip() == "":
            logger.error("[EXTRACT] Text extraction returned empty text")
            raise HTTPException(status_code=400, detail="Could not extract text from PDF. Please ensure the PDF contains readable text.")
        
        logger.info(f"[SUMMARY] Generating summary in {lang}")
        summary = generate_summary(text, lang)
        
        # Check if summary generation was successful
        if summary is None:
            logger.error("[SUMMARY] Summary generation returned None")
            raise HTTPException(status_code=500, detail="Summary generation failed. Please try again.")
        
        summary_length = len(summary) if summary else 0
        logger.info(f"[RESULT] Summary type: {type(summary)}, length: {summary_length}")
        
        if isinstance(summary, dict):
            data = summary
            with open("debug_summary.json", "w") as f:
                json.dump(summary, f, indent=2)
        elif isinstance(summary, str):
            with open("debug_summary_original.json", "w", encoding='utf-8') as f:
                f.write(summary)
            clean_json = clean_json_string(summary)
            with open("debug_summary_cleaned.json", "w", encoding='utf-8') as f:
                f.write(clean_json)
            try:
                data = json.loads(clean_json)
                logger.info("[JSON] Successfully parsed JSON response")
            except json.JSONDecodeError as e:
                logger.error(f"[JSON] JSON parsing failed even after cleaning: {e}")
                logger.error(f"[JSON] Error at position {e.pos}: '{clean_json[max(0, e.pos-10):e.pos+10]}'")
                data = {
                    "Summary": "Error parsing LLM response - using fallback structure",
                    "Clauses": [],
                    "processing_error": str(e),
                    "original_response_length": len(summary)
                }
        else:
            raise TypeError(f"Unexpected summary type: {type(summary)}")
            
        clauses = data.get("Clauses", [])
        logger.info(f"[COMPLIANCE] Processing {len(clauses)} clauses for compliance checking")
        
        try:
            if len(clauses) == 0:
                logger.warning("[COMPLIANCE] No clauses found to process, creating minimal compliance result")
                compliance_results = {
                    "verification_results": [],
                    "risk_explanations": [],
                    "compliance_stats": {
                        "total_clauses": 0,
                        "compliant_count": 0,
                        "non_compliant_count": 0,
                        "high_risk_count": 0,
                        "medium_risk_count": 0,
                        "low_risk_count": 0,
                        "compliance_rate": 0
                    }
                }
            else:
                compliance_agent = ComplianceAgent(llm_client="gemini")
                compliance_results = compliance_agent.ensure_compliance(clauses)
                logger.info(f"[COMPLIANCE] Successfully completed compliance checking for {len(clauses)} clauses")
                
                verification_results = compliance_results.get("verification_results", [])
                risk_explanations = compliance_results.get("risk_explanations", [])
                
                total_clauses = len(verification_results)
                compliant_count = sum(1 for result in verification_results if result.get("is_compliant", False))
                non_compliant_count = total_clauses - compliant_count
                
                high_risk_count = sum(1 for risk in risk_explanations if risk and risk.get("severity") == "High")
                medium_risk_count = sum(1 for risk in risk_explanations if risk and risk.get("severity") == "Medium")
                low_risk_count = sum(1 for risk in risk_explanations if risk and risk.get("severity") == "Low")
                
                compliance_results = {
                    **compliance_results,
                    "compliance_stats": {
                        "total_clauses": total_clauses,
                        "compliant_count": compliant_count,
                        "non_compliant_count": non_compliant_count,
                        "high_risk_count": high_risk_count,
                        "medium_risk_count": medium_risk_count,
                        "low_risk_count": low_risk_count,
                        "compliance_rate": round((compliant_count / total_clauses * 100), 2) if total_clauses > 0 else 0
                    }
                }
        except Exception as e:
            logger.error(f"[COMPLIANCE] Error during compliance checking: {e}\n{traceback.format_exc()}")
            compliance_results = {
                "status": "Compliance checking failed",
                "error": str(e),
                "verification_results": [],
                "risk_explanations": [],
                "compliance_stats": {
                    "total_clauses": len(clauses),
                    "compliant_count": 0,
                    "non_compliant_count": 0,
                    "high_risk_count": 0,
                    "medium_risk_count": 0,
                    "low_risk_count": 0,
                    "compliance_rate": 0
                }
            }

        custom_encoders = {
            np.bool_: bool,
            np.int64: int,
            np.float64: float
        }

        results = {
            "document_id": document_id,
            "summary": data.get("summary", ""),
            "timelines": data.get("Timelines", {}),
            "clauses": clauses,
            "compliance_results": compliance_results,
            "processing_completed_at": datetime.now().isoformat()
        }
        
        logger.info(f"[GCS] Storing processing results for document {document_id}")
        gcs_client.upload_processing_results(document_id, results)
        
        compliance_stats = compliance_results.get("compliance_stats", {})
        completion_metadata = {
            **upload_metadata,
            "processing_status": "completed",
            "processed_at": datetime.now().isoformat(),
            "total_clauses": len(clauses),
            "has_compliance_results": bool(compliance_results),
            "compliance_rate": compliance_stats.get("compliance_rate", 0),
            "compliant_count": compliance_stats.get("compliant_count", 0),
            "non_compliant_count": compliance_stats.get("non_compliant_count", 0),
            "high_risk_count": compliance_stats.get("high_risk_count", 0),
            "medium_risk_count": compliance_stats.get("medium_risk_count", 0),
            "low_risk_count": compliance_stats.get("low_risk_count", 0),
            "overall_score": compliance_stats.get("compliance_rate", 0)
        }
        gcs_client.upload_document_metadata(document_id, completion_metadata)
        
        with open("debug_results.json", "w") as f:
            json.dump(results, f, indent=2)

        logger.info(f"[GCS] Document {document_id} fully processed and stored in GCS bucket: {gcs_client.bucket_name}")
        
        return jsonable_encoder(results, custom_encoder=custom_encoders)

    except Exception as e:
        logger.error(f"[ERROR] Error processing upload: {str(e)}\n{traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Processing error",
                "message": str(e),
                "type": type(e).__name__,
                "file_info": {
                    "filename": file.filename,
                    "content_type": file.content_type,
                    "size": len(content) if 'content' in locals() else "unknown"
                }
            }
        )

# ============================================================================
# DASHBOARD ENDPOINTS
# ============================================================================

@app.get("/api/dashboard/overview")
async def get_dashboard_overview():
    """Get dashboard overview statistics from real GCS data"""
    logger.info("[API] Dashboard overview endpoint accessed")
    try:
        gcs_client = get_gcs_client()
        summary = gcs_client.get_dashboard_summary()
        return {
            "status": "success",
            "data": {
                "totalDocuments": summary["total_documents"],
                "processedDocuments": summary["processed_documents"],
                "complianceRate": summary["total_compliance_rate"],
                "averageScore": summary["total_compliance_rate"],
                "highRiskItems": summary["high_risk_documents"],
                "processingTime": summary["avg_processing_time"],
                "backendHealth": "healthy",
                "lastUpdated": datetime.now().isoformat()
            }
        }
    except Exception as e:
        logger.error(f"[API] Failed to get dashboard overview from GCS: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to get dashboard data: {str(e)}")

@app.get("/api/dashboard/documents")
async def get_documents():
    """Get all processed documents from GCS"""
    logger.info("[API] Documents endpoint accessed: /api/dashboard/documents")
    try:
        gcs_client = get_gcs_client()
        logger.info(f"[GCS] Fetching document list from bucket: {gcs_client.bucket_name}")
        document_ids = gcs_client.list_documents(limit=100)
        
        documents = []
        for doc_id in document_ids:
            metadata = gcs_client.get_document_metadata(doc_id)
            if metadata:
                file_size_mb = round(metadata.get('file_size', 0) / (1024 * 1024), 2)
                high_risk = metadata.get('high_risk_count', 0)
                medium_risk = metadata.get('medium_risk_count', 0)
                compliance_rate = metadata.get('compliance_rate', 0)
                
                if high_risk > 0:
                    risk_level = "high"
                elif medium_risk > 0:
                    risk_level = "medium"
                elif compliance_rate >= 80:
                    risk_level = "low"
                else:
                    risk_level = "medium"

                doc_info = {
                    "id": doc_id,
                    "fileName": metadata.get('filename', 'Unknown'),
                    "fileSize": f"{file_size_mb} MB",
                    "uploadedAt": metadata.get('uploaded_at', datetime.now().isoformat()),
                    "processedAt": metadata.get('processed_at', metadata.get('uploaded_at', datetime.now().isoformat())),
                    "summary": f"Document processed with {metadata.get('total_clauses', 0)} clauses. Compliance rate: {compliance_rate}%",
                    "overallScore": metadata.get('overall_score', compliance_rate),
                    "riskLevel": risk_level,
                    "totalClauses": metadata.get('total_clauses', 0),
                    "compliantClauses": metadata.get('compliant_count', 0),
                    "nonCompliantClauses": metadata.get('non_compliant_count', 0),
                    "highRiskClauses": metadata.get('high_risk_count', 0),
                    "mediumRiskClauses": metadata.get('medium_risk_count', 0),
                    "lowRiskClauses": metadata.get('low_risk_count', 0),
                    "complianceRate": compliance_rate,
                    "status": metadata.get('processing_status', 'unknown'),
                    "language": metadata.get('language', 'English'),
                    "contentType": metadata.get('content_type', 'application/pdf')
                }
                documents.append(doc_info)
        
        documents.sort(key=lambda x: x['uploadedAt'], reverse=True)
        
        logger.info(f"[API] Successfully retrieved {len(documents)} documents")
        return {
            "status": "success",
            "data": documents,
            "total": len(documents)
        }
    except Exception as e:
        logger.error(f"[API] Failed to get documents from GCS: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to get documents: {str(e)}")

@app.get("/api/dashboard/analysis/{document_id}")
async def get_document_analysis(document_id: str):
    """Get detailed analysis for a specific document from GCS"""
    logger.info(f"[API] Document analysis endpoint accessed for document_id: {document_id}")
    try:
        gcs_client = get_gcs_client()
        metadata = gcs_client.get_document_metadata(document_id)
        results = gcs_client.get_processing_results(document_id)
        
        if not metadata or not results:
            logger.error(f"[API] Document {document_id} not found in GCS")
            raise HTTPException(status_code=404, detail=f"Document {document_id} not found")
        
        file_size_mb = round(metadata.get('file_size', 0) / (1024 * 1024), 2)
        clauses = results.get('clauses', [])
        compliance_results = results.get('compliance_results', {})
        verification_results = compliance_results.get('verification_results', [])
        risk_explanations = compliance_results.get('risk_explanations', [])
        compliance_stats = compliance_results.get('compliance_stats', {})
        
        enhanced_clauses = []
        for i, clause in enumerate(clauses):
            verification_result = verification_results[i] if i < len(verification_results) else {}
            risk_explanation = risk_explanations[i] if i < len(risk_explanations) else {}
            
            enhanced_clause = {
                "id": f"clause_{i+1}",
                "text": clause.get('text_en', clause.get('text', f'Clause {i+1}')),
                "isCompliant": verification_result.get('is_compliant', False),
                "confidenceScore": 0.85,
                "riskLevel": risk_explanation.get('severity', 'Unknown').lower() if risk_explanation else 'unknown',
                "riskScore": risk_explanation.get('risk_score', 0) if risk_explanation else 0,
                "category": risk_explanation.get('category', 'General') if risk_explanation else 'General',
                "explanation": verification_result.get('final_reason', 'Analysis completed'),
                "impact": risk_explanation.get('impact', 'No specific impact identified') if risk_explanation else 'No specific impact identified',
                "mitigation": risk_explanation.get('mitigation', 'Review recommended') if risk_explanation else 'Review recommended',
                "matched_rules": verification_result.get('matched_rules', [])
            }
            enhanced_clauses.append(enhanced_clause)
        
        overall_score = compliance_stats.get('compliance_rate', 0)
        compliant_count = compliance_stats.get('compliant_count', 0)
        high_risk_count = compliance_stats.get('high_risk_count', 0)
        medium_risk_count = compliance_stats.get('medium_risk_count', 0)
        low_risk_count = compliance_stats.get('low_risk_count', 0)
        
        analysis_data = {
            "id": document_id,
            "fileName": metadata.get('filename', 'Unknown'),
            "fileSize": f"{file_size_mb} MB",
            "uploadedAt": metadata.get('uploaded_at', datetime.now().isoformat()),
            "processedAt": metadata.get('processed_at', metadata.get('uploaded_at')),
            "summary": results.get('summary', f"Analysis of {metadata.get('filename', 'document')} with {len(clauses)} clauses"),
            "overallScore": overall_score,
            "complianceRate": overall_score,
            "totalClauses": len(clauses),
            "compliantClauses": compliant_count,
            "nonCompliantClauses": len(clauses) - compliant_count,
            "highRiskClauses": high_risk_count,
            "mediumRiskClauses": medium_risk_count,
            "lowRiskClauses": low_risk_count,
            "riskLevel": "high" if high_risk_count > 0 else ("medium" if medium_risk_count > 0 else "low"),
            "status": metadata.get('processing_status', 'completed'),
            "language": metadata.get('language', 'English'),
            "contentType": metadata.get('content_type', 'application/pdf'),
            "clauses": enhanced_clauses,
            "timelines": results.get('timelines', {}),
            "compliance_results": compliance_results,
            "compliance_stats": compliance_stats,
            "processing_completed_at": results.get('processing_completed_at'),
            "gcs_stored": True
        }
        
        return {
            "status": "success",
            "data": analysis_data
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[API] Failed to get analysis for {document_id} from GCS: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to get analysis: {str(e)}")

@app.get("/api/dashboard/reports")
async def get_reports():
    """Get compliance reports"""
    logger.info("[API] Reports endpoint accessed")
    reports = [
        {
            "id": "report_001",
            "title": "Monthly Compliance Report",
            "type": "compliance",
            "description": "Comprehensive compliance analysis for the month",
            "generatedAt": datetime.now().isoformat(),
            "status": "completed",
            "downloadUrl": "/api/reports/download/report_001"
        },
        {
            "id": "report_002",
            "title": "Risk Assessment Report",
            "type": "risk",
            "description": "High-risk clauses and mitigation recommendations",
            "generatedAt": (datetime.now() - timedelta(hours=1)).isoformat(),
            "status": "completed",
            "downloadUrl": "/api/reports/download/report_002"
        }
    ]
    return {
        "status": "success",
        "data": reports,
        "total": len(reports)
    }

@app.post("/api/dashboard/reports/generate")
async def generate_report(report_type: str = "compliance"):
    """Generate a new compliance report"""
    logger.info(f"[API] Generating report of type: {report_type}")
    report_id = f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    return {
        "status": "success",
        "message": f"{report_type.title()} report generation started",
        "reportId": report_id,
        "estimatedTime": "30 seconds"
    }

@app.get("/api/dashboard/reports/export/compliance")
async def export_compliance_reports(start_date: Optional[str] = None, end_date: Optional[str] = None):
    """Export detailed compliance reports from GCS"""
    logger.info(f"[API] Exporting compliance reports: start_date={start_date}, end_date={end_date}")
    try:
        gcs_client = get_gcs_client()
        report_data = gcs_client.export_compliance_reports(start_date, end_date)
        if "error" in report_data:
            raise HTTPException(status_code=500, detail=report_data["error"])
        return {
            "status": "success",
            "data": report_data,
            "export_format": "detailed_json"
        }
    except Exception as e:
        logger.error(f"[API] Failed to export compliance reports: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/dashboard/reports/export/risk-analysis")
async def export_risk_analysis(start_date: Optional[str] = None, end_date: Optional[str] = None):
    """Export detailed risk analysis reports from GCS"""
    logger.info(f"[API] Exporting risk analysis: start_date={start_date}, end_date={end_date}")
    try:
        gcs_client = get_gcs_client()
        report_data = gcs_client.export_risk_analysis(start_date, end_date)
        if "error" in report_data:
            raise HTTPException(status_code=500, detail=report_data["error"])
        return {
            "status": "success",
            "data": report_data,
            "export_format": "detailed_json"
        }
    except Exception as e:
        logger.error(f"[API] Failed to export risk analysis: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/dashboard/reports/export/trend-analysis")
async def export_trend_analysis(period: str = "30d"):
    """Export trend analysis reports from GCS"""
    logger.info(f"[API] Exporting trend analysis for period: {period}")
    try:
        gcs_client = get_gcs_client()
        report_data = gcs_client.export_trend_analysis(period)
        if "error" in report_data:
            raise HTTPException(status_code=500, detail=report_data["error"])
        return {
            "status": "success",
            "data": report_data,
            "export_format": "detailed_json"
        }
    except Exception as e:
        logger.error(f"[API] Failed to export trend analysis: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/dashboard/reports/export/custom")
async def export_custom_report(filters: Dict[str, Any]):
    """Export custom reports based on filters from GCS"""
    logger.info(f"[API] Exporting custom report with filters: {filters}")
    try:
        gcs_client = get_gcs_client()
        report_data = gcs_client.export_custom_report(filters)
        if "error" in report_data:
            raise HTTPException(status_code=500, detail=report_data["error"])
        return {
            "status": "success",
            "data": report_data,
            "export_format": "detailed_json"
        }
    except Exception as e:
        logger.error(f"[API] Failed to export custom report: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/dashboard/analyze/{document_id}")
async def analyze_document_compliance(document_id: str):
    """Perform real-time compliance analysis on a stored document"""
    logger.info(f"[API] Analyzing document compliance for document_id: {document_id}")
    try:
        gcs_client = get_gcs_client()
        analysis_result = gcs_client.analyze_document_compliance(document_id)
        if "error" in analysis_result:
            raise HTTPException(status_code=400, detail=analysis_result["error"])
        return {
            "status": "success",
            "data": analysis_result
        }
    except Exception as e:
        logger.error(f"[API] Failed to analyze document {document_id}: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/dashboard/analyze-all")
async def analyze_all_documents(limit: int = 10):
    """Perform comprehensive compliance analysis on all stored documents"""
    logger.info(f"[API] Analyzing all documents with limit: {limit}")
    try:
        gcs_client = get_gcs_client()
        analysis_result = gcs_client.analyze_all_documents_compliance(limit=limit)
        if "error" in analysis_result:
            raise HTTPException(status_code=500, detail=analysis_result["error"])
        return {
            "status": "success",
            "data": analysis_result
        }
    except Exception as e:
        logger.error(f"[API] Failed to analyze all documents: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/dashboard/refresh-analytics")
async def refresh_dashboard_analytics():
    """Refresh all dashboard analytics with real-time data"""
    logger.info("[API] Refreshing dashboard analytics")
    try:
        gcs_client = get_gcs_client()
        comprehensive_analysis = gcs_client.analyze_all_documents_compliance(limit=50)
        if "error" in comprehensive_analysis:
            raise HTTPException(status_code=500, detail=comprehensive_analysis["error"])
        summary = comprehensive_analysis.get("summary", {})
        analytics_data = {
            "complianceTrend": [
                {
                    "date": datetime.now().strftime("%Y-%m-%d"),
                    "score": summary.get("total_compliance_rate", 0)
                }
            ],
            "riskDistribution": {
                "high": summary.get("high_risk_documents", 0),
                "medium": max(1, summary.get("analyzed_documents", 1) - summary.get("high_risk_documents", 0) - 1),
                "low": 1,
                "compliant": summary.get("analyzed_documents", 0) - summary.get("high_risk_documents", 0)
            },
            "processingStats": {
                "averageTime": 2000,
                "successRate": round((summary.get("analyzed_documents", 0) / summary.get("total_documents", 1)) * 100, 1),
                "totalProcessed": summary.get("analyzed_documents", 0)
            },
            "complianceAreas": {
                "Legal Compliance": summary.get("total_compliance_rate", 0),
                "Financial Terms": max(0, summary.get("total_compliance_rate", 0) - 5),
                "Risk Disclosure": min(100, summary.get("total_compliance_rate", 0) + 10),
                "Regulatory Requirements": min(100, summary.get("total_compliance_rate", 0) + 15)
            },
            "lastUpdated": datetime.now().isoformat()
        }
        return {
            "status": "success",
            "message": "Dashboard analytics refreshed with real-time data",
            "data": analytics_data
        }
    except Exception as e:
        logger.error(f"[API] Failed to refresh dashboard analytics: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/dashboard/notifications")
async def get_notifications():
    """Get user notifications"""
    logger.info("[API] Notifications endpoint accessed")
    try:
        gcs_client = get_gcs_client()
        document_ids = gcs_client.list_documents(limit=20)
        notifications = []
        notification_id_counter = 1
        for doc_id in document_ids:
            metadata = gcs_client.get_document_metadata(doc_id)
            if metadata:
                filename = metadata.get('filename', 'Unknown Document')
                processing_status = metadata.get('processing_status', 'unknown')
                high_risk_count = metadata.get('high_risk_count', 0)
                compliance_rate = metadata.get('compliance_rate', 0)
                uploaded_at = metadata.get('uploaded_at')
                processed_at = metadata.get('processed_at')
                if high_risk_count > 0:
                    notifications.append({
                        "id": f"notif_{notification_id_counter:03d}",
                        "type": "warning",
                        "title": "High Risk Clause Detected",
                        "message": f"{high_risk_count} high-risk clause(s) detected in {filename}",
                        "timestamp": processed_at or uploaded_at or datetime.now().isoformat(),
                        "read": False,
                        "priority": "high",
                        "documentId": doc_id
                    })
                    notification_id_counter += 1
                if processing_status == 'completed':
                    notifications.append({
                        "id": f"notif_{notification_id_counter:03d}",
                        "type": "success",
                        "title": "Document Processing Complete",
                        "message": f"{filename} has been successfully analyzed with {compliance_rate}% compliance",
                        "timestamp": processed_at or datetime.now().isoformat(),
                        "read": False,
                        "priority": "medium",
                        "documentId": doc_id
                    })
                    notification_id_counter += 1
                if compliance_rate < 70 and processing_status == 'completed':
                    notifications.append({
                        "id": f"notif_{notification_id_counter:03d}",
                        "type": "error",
                        "title": "Low Compliance Score",
                        "message": f"{filename} has a compliance score of {compliance_rate}%. Review required.",
                        "timestamp": processed_at or datetime.now().isoformat(),
                        "read": False,
                        "priority": "high",
                        "documentId": doc_id
                    })
                    notification_id_counter += 1
        notifications.sort(key=lambda x: x['timestamp'], reverse=True)
        notifications = notifications[:10]
        unread_count = len([n for n in notifications if not n["read"]])
        return {
            "status": "success",
            "data": notifications,
            "unreadCount": unread_count,
            "total": len(notifications)
        }
    except Exception as e:
        logger.error(f"[API] Failed to get notifications from GCS: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to get notifications: {str(e)}")

@app.put("/api/dashboard/notifications/{notification_id}/read")
async def mark_notification_read(notification_id: str):
    """Mark a notification as read"""
    logger.info(f"[API] Marking notification as read: {notification_id}")
    return {
        "status": "success",
        "message": f"Notification {notification_id} marked as read"
    }

@app.get("/api/dashboard/timeline")
async def get_timeline(days: int = 30):
    """Get processing timeline events from real GCS data"""
    logger.info(f"[API] Timeline endpoint accessed for last {days} days")
    try:
        gcs_client = get_gcs_client()
        document_ids = gcs_client.list_documents(limit=20)
        timeline_events = []
        event_id_counter = 1
        
        # Calculate cutoff date for filtering - be more lenient for demo
        cutoff_date = datetime.now() - timedelta(days=max(days, 30))  # Always include last 30 days minimum
        
        for doc_id in document_ids:
            metadata = gcs_client.get_document_metadata(doc_id)
            if metadata:
                filename = metadata.get('filename', 'Unknown Document')
                uploaded_at = metadata.get('uploaded_at')
                processed_at = metadata.get('processed_at')
                processing_status = metadata.get('processing_status', 'unknown')
                compliance_rate = metadata.get('compliance_rate', 0)
                high_risk = metadata.get('high_risk_count', 0)
                medium_risk = metadata.get('medium_risk_count', 0)
                
                logger.info(f"[TIMELINE] Processing doc {doc_id}: status={processing_status}, compliance={compliance_rate}, high_risk={high_risk}")
                
                # Always create document upload event
                if uploaded_at:
                    try:
                        uploaded_date = datetime.fromisoformat(uploaded_at.replace('Z', '+00:00'))
                        timeline_events.append({
                            "id": f"event_{event_id_counter:03d}",
                            "type": "document",  # This matches frontend filter
                            "title": f"Document Uploaded: {filename}",
                            "description": f"{filename} uploaded to GCS for processing. Status: {processing_status}",
                            "timestamp": uploaded_at,
                            "documentId": doc_id,
                            "status": "completed" if processing_status == "completed" else "processing"
                        })
                        event_id_counter += 1
                        logger.info(f"[TIMELINE] Added document event for {doc_id}")
                    except Exception as e:
                        logger.warning(f"[TIMELINE] Failed to parse uploaded_at for {doc_id}: {e}")
                
                # Always create compliance event for completed documents
                if processing_status == 'completed':
                    event_timestamp = processed_at or uploaded_at or datetime.now().isoformat()
                    try:
                        # Always add a compliance event
                        timeline_events.append({
                            "id": f"event_{event_id_counter:03d}",
                            "type": "compliance",  # This matches frontend filter
                            "title": f"Analysis Complete: {filename}",
                            "description": f"SEBI compliance analysis finished with {compliance_rate}% compliance rate. High risk: {high_risk}, Medium risk: {medium_risk}",
                            "timestamp": event_timestamp,
                            "documentId": doc_id,
                            "status": "completed"
                        })
                        event_id_counter += 1
                        logger.info(f"[TIMELINE] Added compliance event for {doc_id}")
                        
                        # Also add a risk event if high risk found OR medium risk found
                        if high_risk > 0 or medium_risk > 0:
                            timeline_events.append({
                                "id": f"event_{event_id_counter:03d}",
                                "type": "risk",  # This matches frontend filter
                                "title": f"Risk Detected: {filename}",
                                "description": f"Risk violations found: {high_risk} high-risk issues, {medium_risk} medium-risk issues. Compliance: {compliance_rate}%",
                                "timestamp": event_timestamp,
                                "documentId": doc_id,
                                "status": "completed"
                            })
                            event_id_counter += 1
                            logger.info(f"[TIMELINE] Added risk event for {doc_id} (high={high_risk}, medium={medium_risk})")
                    except Exception as e:
                        logger.warning(f"[TIMELINE] Failed to create compliance/risk events for {doc_id}: {e}")
                
                # Create system event for processing status
                if processing_status in ['processing', 'started', 'pending']:
                    try:
                        event_timestamp = uploaded_at or datetime.now().isoformat()
                        timeline_events.append({
                            "id": f"event_{event_id_counter:03d}",
                            "type": "system",  # This matches frontend filter for system events
                            "title": f"Document Processing: {filename}",
                            "description": f"Currently analyzing {filename} for SEBI compliance. Status: {processing_status}",
                            "timestamp": event_timestamp,
                            "documentId": doc_id,
                            "status": "processing"
                        })
                        event_id_counter += 1
                        logger.info(f"[TIMELINE] Added system event for {doc_id}")
                    except Exception as e:
                        logger.warning(f"[TIMELINE] Failed to create system event for {doc_id}: {e}")
        
        # Sort by timestamp descending (newest first)
        timeline_events.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        
        logger.info(f"[API] Timeline: Returning {len(timeline_events)} events (filtered for last {days} days)")
        return {
            "status": "success",
            "data": timeline_events[:10],
            "total": len(timeline_events)
        }
    except Exception as e:
        logger.error(f"[API] Failed to get timeline from GCS: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to get timeline: {str(e)}")

@app.get("/api/dashboard/analytics")
async def get_analytics():
    """Get analytics data for charts and metrics from real GCS data"""
    logger.info("[API] Analytics endpoint accessed")
    try:
        gcs_client = get_gcs_client()
        document_ids = gcs_client.list_documents(limit=100)
        compliance_trend_data = {}
        compliance_rates = []
        risk_distribution = {"high": 0, "medium": 0, "low": 0, "compliant": 0}
        processing_times = []
        total_processed = 0
        successful_processing = 0
        for doc_id in document_ids:
            metadata = gcs_client.get_document_metadata(doc_id)
            if metadata:
                processing_status = metadata.get('processing_status')
                if processing_status == 'completed':
                    total_processed += 1
                    successful_processing += 1
                    processed_date = metadata.get('processed_at') or metadata.get('uploaded_at')
                    if processed_date:
                        try:
                            date_obj = datetime.fromisoformat(processed_date.replace('Z', '+00:00'))
                            date_str = date_obj.strftime("%Y-%m-%d")
                            compliance_rate = metadata.get('compliance_rate', 0)
                            if date_str not in compliance_trend_data:
                                compliance_trend_data[date_str] = []
                            compliance_trend_data[date_str].append(compliance_rate)
                            compliance_rates.append(compliance_rate)
                        except:
                            pass
                    high_risk = metadata.get('high_risk_count', 0)
                    medium_risk = metadata.get('medium_risk_count', 0)
                    low_risk = metadata.get('low_risk_count', 0)
                    compliance_rate = metadata.get('compliance_rate', 0)
                    if high_risk > 0:
                        risk_distribution["high"] += high_risk
                    if medium_risk > 0:
                        risk_distribution["medium"] += medium_risk
                    if low_risk > 0:
                        risk_distribution["low"] += low_risk
                    if compliance_rate >= 90:
                        risk_distribution["compliant"] += 1
                    processing_times.append(2000 + (high_risk * 300) + (medium_risk * 150))
                elif processing_status in ['processing', 'started']:
                    total_processed += 1
        compliance_trend = []
        for i in range(6, -1, -1):
            date = (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d")
            if date in compliance_trend_data:
                avg_score = sum(compliance_trend_data[date]) / len(compliance_trend_data[date])
                compliance_trend.append({"date": date, "score": round(avg_score, 1)})
            else:
                prev_score = compliance_trend[-1]["score"] if compliance_trend else 85
                compliance_trend.append({"date": date, "score": prev_score})
        success_rate = round((successful_processing / total_processed * 100), 1) if total_processed > 0 else 0
        avg_processing_time = int(sum(processing_times) / len(processing_times)) if processing_times else 2450
        analytics_data = {
            "complianceTrend": compliance_trend,
            "riskDistribution": risk_distribution,
            "processingStats": {
                "averageTime": avg_processing_time,
                "successRate": success_rate,
                "totalProcessed": total_processed
            },
            "complianceAreas": {
                "Legal Compliance": round(sum(compliance_rates) / len(compliance_rates), 1) if compliance_rates else 85,
                "Financial Terms": round((sum(compliance_rates) / len(compliance_rates) - 5), 1) if compliance_rates else 80,
                "Risk Disclosure": round((sum(compliance_rates) / len(compliance_rates) + 3), 1) if compliance_rates else 88,
                "Regulatory Requirements": round((sum(compliance_rates) / len(compliance_rates) + 6), 1) if compliance_rates else 91
            }
        }
        return {
            "status": "success",
            "data": analytics_data
        }
    except Exception as e:
        logger.error(f"[API] Failed to get analytics from GCS: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to get analytics: {str(e)}")

@app.get("/api/llm/providers")
async def get_llm_providers():
    """Get available LLM providers"""
    logger.info("[API] LLM providers endpoint accessed")
    try:
        return {
            "providers": [
                {
                    "id": "gemini",
                    "name": "Google Gemini",
                    "status": "available" if os.getenv("GEMINI_API_KEY") else "unavailable",
                    "description": "Google's Gemini Pro integrated with FastAPI backend",
                    "capabilities": ["text-processing", "compliance-analysis", "summarization"]
                },
                {
                    "id": "claude",
                    "name": "Anthropic Claude",
                    "status": "available" if os.getenv("ANTHROPIC_API_KEY") else "unavailable",
                    "description": "Anthropic's Claude integrated with FastAPI backend",
                    "capabilities": ["text-processing", "compliance-analysis", "legal-review"]
                },
                {
                    "id": "openai",
                    "name": "OpenAI GPT",
                    "status": "available" if os.getenv("OPENAI_API_KEY") else "unavailable",
                    "description": "OpenAI's GPT models integrated with FastAPI backend",
                    "capabilities": ["text-processing", "compliance-analysis", "document-analysis"]
                },
                {
                    "id": "mistral",
                    "name": "Mistral AI",
                    "status": "available" if os.getenv("MISTRAL_API_KEY") else "unavailable",
                    "description": "Mistral AI models integrated with FastAPI backend",
                    "capabilities": ["text-processing", "compliance-analysis"]
                },
                {
                    "id": "vertex_ai",
                    "name": "Google Vertex AI",
                    "status": "available" if os.getenv("GOOGLE_APPLICATION_CREDENTIALS") else "unavailable",
                    "description": "Google Vertex AI models integrated with FastAPI backend",
                    "capabilities": ["text-processing", "compliance-analysis", "enterprise-features"]
                }
            ],
            "source": "fastapi_backend_real",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"[LLM] Error getting providers: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get LLM providers: {str(e)}")

@app.delete("/api/dashboard/documents/{document_id}")
async def delete_document(document_id: str):
    """Delete a specific document"""
    logger.info(f"[API] Delete document endpoint accessed for: {document_id}")
    try:
        gcs_client = get_gcs_client()
        
        # Delete document from GCS using the existing method
        success = gcs_client.delete_document(document_id)
        
        if success:
            logger.info(f"[GCS] Document {document_id} deleted successfully")
            return {
                "success": True,
                "documentId": document_id,
                "message": f"Document {document_id} deleted successfully",
                "timestamp": datetime.now().isoformat(),
                "source": "fastapi_backend_real"
            }
        else:
            raise HTTPException(status_code=404, detail=f"Document {document_id} not found or could not be deleted")
            
    except Exception as e:
        logger.error(f"[DELETE] Error deleting document {document_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete document: {str(e)}")

@app.post("/api/dashboard/clear-all")
async def clear_all_data():
    """Clear all data from the system"""
    logger.info("[API] Clear all data endpoint accessed")
    try:
        gcs_client = get_gcs_client()
        
        # Get list of all documents first
        try:
            documents = gcs_client.list_documents()
            deleted_count = 0
            
            for doc_metadata in documents:
                doc_id = doc_metadata.get('document_id')
                if doc_id:
                    try:
                        success = gcs_client.delete_document(doc_id)
                        if success:
                            deleted_count += 1
                    except Exception as doc_error:
                        logger.warning(f"[CLEAR] Error deleting document {doc_id}: {doc_error}")
                        continue
            
            logger.info(f"[CLEAR] Cleared {deleted_count} documents successfully")
        except Exception as gcs_error:
            logger.warning(f"[CLEAR] Error during GCS cleanup: {gcs_error}")
            deleted_count = 0
        
        return {
            "success": True,
            "message": f"Successfully cleared {deleted_count} documents",
            "deletedCount": deleted_count,
            "timestamp": datetime.now().isoformat(),
            "source": "fastapi_backend_real"
        }
    except Exception as e:
        logger.error(f"[CLEAR] Error clearing all data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to clear all data: {str(e)}")

@app.get("/test")
async def test_endpoint():
    """Test endpoint for deployment verification"""
    logger.info("[API] Test endpoint accessed")
    return {
        "message": "Backend deployment test successful",
        "timestamp": datetime.now().isoformat(),
        "gcp_configured": bool(os.getenv("GCS_BUCKET_NAME")),
        "gemini_configured": bool(os.getenv("GEMINI_API_KEY")),
        "environment": os.getenv("ENVIRONMENT", "development"),
        "cors_origins": cors_origins
    }

@app.post("/api/compliance/chat")
async def compliance_chat(request: dict):
    """
    Conversational compliance assistant endpoint
    """
    logger.info("[CHAT] Compliance chat request received")
    try:
        from src.compliance_checker.conversational_agent import ConversationalComplianceAgent
        
        message = request.get("message", "")
        session_id = request.get("session_id", "default")
        document_context = request.get("document_context", None)
        
        if not message:
            raise HTTPException(status_code=400, detail="Message is required")
        
        # Initialize conversational agent
        chat_agent = ConversationalComplianceAgent()
        
        # Get response
        response = chat_agent.chat(
            user_message=message,
            session_id=session_id,
            document_context=document_context
        )
        
        logger.info(f"[CHAT] Response generated successfully for session {session_id}")
        return response
        
    except Exception as e:
        logger.error(f"[CHAT] Error: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Chat error: {str(e)}")

@app.post("/api/sebi/update-regulations")
async def update_sebi_regulations():
    """
    Trigger SEBI regulation update check and indexing
    """
    logger.info("[SEBI-UPDATE] Manual update triggered")
    try:
        from src.compliance_checker.sebi_rule_updater import sebi_updater
        
        # Run update cycle
        results = sebi_updater.run_update_cycle()
        
        logger.info(f"[SEBI-UPDATE] Update cycle completed: {results}")
        return {
            "success": True,
            "results": results,
            "message": "SEBI regulation update completed"
        }
        
    except Exception as e:
        logger.error(f"[SEBI-UPDATE] Error: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Update error: {str(e)}")

@app.get("/api/sebi/check-updates")
async def check_sebi_updates():
    """
    Check for SEBI regulation updates without indexing
    """
    logger.info("[SEBI-UPDATE] Checking for updates")
    try:
        from src.compliance_checker.sebi_rule_updater import sebi_updater
        
        # Check for updates only
        updates = sebi_updater.check_for_updates()
        
        return {
            "success": True,
            "updates": updates,
            "message": f"Found {updates.get('total_updates', 0)} new updates"
        }
        
    except Exception as e:
        logger.error(f"[SEBI-UPDATE] Error: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Check error: {str(e)}")

@app.post("/api/notifications/test")
async def test_email_notification(request: dict):
    """
    Test email notification system
    """
    logger.info("[NOTIFICATIONS] Test email requested")
    try:
        from src.notifications.email_service import email_service
        
        to_emails = request.get("to_emails", [])
        if not to_emails:
            raise HTTPException(status_code=400, detail="to_emails is required")
        
        # Send test notification
        success = email_service.send_processing_complete(
            to_emails=to_emails,
            document_name="Test Document.pdf",
            document_id="test_123",
            compliance_rate=85.5,
            risk_level="medium"
        )
        
        return {
            "success": success,
            "message": "Test email sent" if success else "Email sending is disabled or failed"
        }
        
    except Exception as e:
        logger.error(f"[NOTIFICATIONS] Error: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Notification error: {str(e)}")

@app.get("/debug")
async def debug_endpoint():
    """Debug endpoint for troubleshooting deployment issues"""
    logger.info("[API] Debug endpoint accessed")
    try:
        import sys
        import os
        return {
            "status": "debug_success",
            "python_version": sys.version,
            "environment": os.getenv("ENVIRONMENT", "not_set"),
            "working_directory": os.getcwd(),
            "cors_origins": cors_origins,
            "gcs_bucket": os.getenv("GCS_BUCKET_NAME", "not_set"),
            "available_modules": [
                "fastapi" in sys.modules,
                "uvicorn" in sys.modules,
                "google" in str(sys.modules.keys())
            ],
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"[DEBUG] Error in debug endpoint: {e}\n{traceback.format_exc()}")
        return {
            "status": "debug_error",
            "error": str(e),
            "error_type": type(e).__name__,
            "timestamp": datetime.now().isoformat()
        }


@app.get("/notify/")
def notify():
    clause = {
    "id": "CL-2025-001",
    "title": "Data Retention Policy",
    "owner_email": "mahi.dhuwaviya.04@gmail.com",
    "risk_score": 92,  # High risk
    "deadline_days": 1,  # Due in 1 day
    "category": "Compliance",
    "status": "Pending Review",
    "last_updated": "2025-10-27",
    "notes": "Retention period exceeds legal limit; flagged for urgent revision."
}
    if is_high_risk(clause):
        notification_queue.put({
            "to_email": clause["owner_email"],
            "subject": " High-Risk Clause Alert",
            "body": f"Clause {clause['id']} is high-risk and due in {clause['deadline_days']} days."
        })
    return {"status": "Notification queued"}

# Alias for /api/compliance/chat
@app.post("/api/chat")
async def chat_alias(request: dict):
    """Alias endpoint for /api/compliance/chat"""
    return await compliance_chat(request)

@app.get("/api/chat")
async def chat_get(session_id: Optional[str] = None):
    """Get chat session history"""
    logger.info(f"[CHAT] Get chat session: {session_id}")
    return {
        "session_id": session_id or "default",
        "messages": [],
        "message": "Chat session retrieved"
    }

@app.get("/api/companies")
async def get_companies(user_id: Optional[str] = None):
    """Get companies list - stub endpoint"""
    logger.info(f"[COMPANIES] Get companies for user: {user_id}")
    return {
        "companies": [],
        "total": 0,
        "message": "No companies found. This endpoint is not implemented for RegLex compliance system."
    }

@app.get("/api/transactions")
async def get_transactions(page: int = 0, pageSize: int = 1000):
    """Get transactions list - stub endpoint"""
    logger.info(f"[TRANSACTIONS] Get transactions: page={page}, pageSize={pageSize}")
    return {
        "transactions": [],
        "total": 0,
        "page": page,
        "pageSize": pageSize,
        "message": "No transactions found. This endpoint is not implemented for RegLex compliance system."
    }

@app.post("/api/voice/speak")
async def voice_speak(request: dict):
    """
    Generate speech from text using ElevenLabs API
    """
    logger.info("[VOICE] Speak request received")
    try:
        import requests
        
        # Add basic CORS headers manually if needed
        response_headers = {
            "Access-Control-Allow-Origin": "*",  # Or specific origin
        }
        
        text = request.get("text", "")
        voice_id = request.get("voice_id", "21m00Tcm4TlvDq8ikWAM")  # Rachel voice
        
        if not text:
            raise HTTPException(status_code=400, detail="Text is required")
            
        api_key = os.getenv("ELEVENLABS_API_KEY")
        if not api_key:
            # Fallback for hackathon if env var missing
            api_key = os.getenv("ELEVENLABS_API_KEY")
            
        url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
        
        headers = {
            "Accept": "audio/mpeg",
            "Content-Type": "application/json",
            "xi-api-key": api_key
        }
        
        data = {
            "text": text,
            "model_id": "eleven_monolingual_v1",
            "voice_settings": {
                "stability": 0.5,
                "similarity_boost": 0.5
            }
        }
        
        response = requests.post(url, json=data, headers=headers)
        
        if response.status_code != 200:
            logger.error(f"[VOICE] ElevenLabs error: {response.text}")
            raise HTTPException(status_code=response.status_code, detail=f"ElevenLabs error: {response.text}")
            
        return JSONResponse(
            content=response.content.hex(),  # Return hex string because JSONResponse cannot handle bytes directly
            media_type="application/json",
            headers=response_headers
        )
            
    except Exception as e:
        logger.error(f"[VOICE] Error generating speech: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Voice generation error: {str(e)}")

@app.post("/api/voice")
async def voice_post(request: dict):
    """Voice endpoint - stub endpoint"""
    # ... (Keep existing stub or redirect to speak if needed)
    logger.info("[VOICE] Voice request received (stub)")
    session_id = request.get("session_id", f"voice_{int(datetime.now().timestamp() * 1000)}")
    return {
        "session_id": session_id,
        "message": "Voice processing is not implemented for RegLex compliance system.",
        "status": "not_implemented"
    }

@app.post("/api/voice/transcribe")
async def voice_transcribe(file: UploadFile = File(...)):
    """
    Transcribe audio/video using ElevenLabs Scribe API
    """
    logger.info(f"[VOICE] Transcribe request: {file.filename}")
    try:
        api_key = os.getenv("ELEVENLABS_API_KEY")
        url = "https://api.elevenlabs.io/v1/speech-to-text"
        
        content = await file.read()
        
        files = {
            'file': (file.filename, content, file.content_type)
        }
        data = {
            'model_id': 'scribe_v1',
            'tag_audio_events': 'true'
        }
        headers = {
            'xi-api-key': api_key
        }
        
        response = requests.post(url, headers=headers, files=files, data=data)
        
        if response.status_code != 200:
            logger.error(f"[VOICE] ElevenLabs STT error: {response.text}")
            raise HTTPException(status_code=response.status_code, detail=f"Transcription failed: {response.text}")
            
        return response.json()
        
    except Exception as e:
        logger.error(f"[VOICE] Error transcribing: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/voice/interact")
async def voice_interact(file: UploadFile = File(...), document_id: Optional[str] = Form(None)):
    """
    Interactive Voice Assistant: Audio -> STT -> Gemini -> TTS -> Audio
    """
    logger.info(f"[VOICE] Interact request: {file.filename}")
    try:
        # 1. Transcribe audio
        transcription_result = await voice_transcribe(file)
        user_text = transcription_result.get("text", "")
        
        if not user_text:
            return {"error": "Could not understand audio"}
            
        # 2. Get AI Response (Gemini)
        import google.generativeai as genai
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            api_key = os.getenv("GEMINI_API_KEY_2") # Fallback
        
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-2.5-flash")
        
        context = ""
        if document_id:
            gcs_client = get_gcs_client()
            results = gcs_client.get_processing_results(document_id)
            if results:
                context = f"Context from document: {results.get('summary', '')}"
        
        prompt = f"""
        You are RegLex AI, a helpful compliance assistant. 
        {context}
        
        User asked: {user_text}
        
        Provide a concise, helpful answer that can be spoken clearly.
        """
        
        ai_response = model.generate_content(prompt)
        ai_response_text = ai_response.text
        
        # 3. Convert AI response to Speech (TTS)
        voice_response = await voice_speak({"text": ai_response_text})
        
        return {
            "user_text": user_text,
            "ai_text": ai_response_text,
            "audio_hex": voice_response.body.decode().strip('"'), # Extract hex string from JSONResponse
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"[VOICE] Interact error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/voice/briefing/{document_id}")
async def get_document_briefing(document_id: str):
    """
    Generate an executive audio briefing for a document
    """
    logger.info(f"[VOICE] Briefing request for {document_id}")
    try:
        gcs_client = get_gcs_client()
        metadata = gcs_client.get_document_metadata(document_id)
        results = gcs_client.get_processing_results(document_id)
        
        if not results:
            raise HTTPException(status_code=404, detail="Document analysis not found")
            
        summary = results.get("summary", "")
        compliance_rate = results.get("compliance_rate", 0)
        high_risk = results.get("high_risk_count", 0)
        
        # Create a "script" for the audio briefing
        script_prompt = f"""
        You are an executive compliance commentator. 
        Create a 30-second engaging audio briefing script based on this:
        Document: {metadata.get('filename', 'Unknown')}
        Compliance Rate: {compliance_rate}%
        High Risk Issues: {high_risk}
        Summary: {summary}
        
        Make it sound professional, informative, and urgent where necessary.
        Return ONLY the spoken script.
        """
        
        import google.generativeai as genai
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            api_key = os.getenv("GEMINI_API_KEY_2")
            
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-2.5-flash")
        
        briefing_response = model.generate_content(script_prompt)
        briefing_script = briefing_response.text
        
        # Convert to speech
        voice_response = await voice_speak({"text": briefing_script})
        
        return {
            "script": briefing_script,
            "audio_hex": voice_response.body.decode().strip('"'),
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"[VOICE] Briefing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/video/briefing/{document_id}")
async def generate_video_briefing(document_id: str):
    """
    Generate an AI Video Briefing for a document: 
    Gemini Script -> ElevenLabs Audio -> FFmpeg Synthesis -> MP4 Video
    """
    logger.info(f"[VIDEO] Video briefing request for {document_id}")
    try:
        # 1. Get audio briefing data
        briefing_data = await get_document_briefing(document_id)
        audio_hex = briefing_data.get("audio_hex")
        script = briefing_data.get("script")
        
        if not audio_hex:
            raise HTTPException(status_code=500, detail="Failed to generate audio for video")
            
        gcs_client = get_gcs_client()
        metadata = gcs_client.get_document_metadata(document_id)
        filename = metadata.get('filename', 'RegLex Compliance Report')
        
        # 2. Create a temporary workspace
        with tempfile.TemporaryDirectory() as tmpdir:
            audio_path = os.path.join(tmpdir, "audio.mp3")
            image_path = os.path.join(tmpdir, "background.png")
            video_path = os.path.join(tmpdir, "briefing.mp4")
            
            # Save audio
            u8 = bytearray.fromhex(audio_hex)
            with open(audio_path, "wb") as f:
                f.write(u8)
                
            # 3. Generate dynamic background image
            # Create a 1280x720 canvas
            img = Image.new('RGB', (1280, 720), color=(15, 23, 42)) # Slate 900
            draw = ImageDraw.Draw(img)
            
            # Add branding
            draw.rectangle([40, 40, 1240, 680], outline=(16, 185, 129), width=10) # Emerald 500 border
            
            # Text rendering (simple fallback if fonts missing)
            try:
                # In most linux containers, fonts are in /usr/share/fonts
                font_title = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 60)
                font_body = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 30)
            except:
                font_title = ImageFont.load_default()
                font_body = ImageFont.load_default()
                
            draw.text((100, 100), "RegLex AI Briefing", fill=(16, 185, 129), font=font_title)
            draw.text((100, 200), f"Document: {filename}", fill=(255, 255, 255), font=font_body)
            draw.text((100, 260), f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}", fill=(148, 163, 184), font=font_body)
            
            # Summary snippet
            wrapped_text = ""
            words = script.split()
            line = ""
            for word in words[:50]: # First 50 words
                if len(line + word) < 60:
                    line += word + " "
                else:
                    wrapped_text += line + "\n"
                    line = word + " "
            wrapped_text += line
            
            draw.text((100, 350), wrapped_text, fill=(226, 232, 240), font=font_body)
            
            img.save(image_path)
            
            # 4. Use FFmpeg to combine image and audio
            cmd = [
                "ffmpeg", "-y",
                "-loop", "1", "-i", image_path,
                "-i", audio_path,
                "-c:v", "libx264", "-t", "60",
                "-pix_fmt", "yuv420p",
                "-vf", "scale=1280:720",
                "-shortest",
                video_path
            ]
            
            process = subprocess.run(cmd, capture_output=True, text=True)
            if process.returncode != 0:
                logger.error(f"[VIDEO] FFmpeg error: {process.stderr}")
                raise HTTPException(status_code=500, detail=f"Video synthesis failed: {process.stderr}")
                
            # 5. Read back the video and upload to GCS
            with open(video_path, "rb") as f:
                video_content = f.read()
                
            video_gcs_path = f"videos/{document_id}_briefing.mp4"
            bucket = gcs_client.bucket
            blob = bucket.blob(video_gcs_path)
            blob.upload_from_string(video_content, content_type="video/mp4")
            
            try:
                blob.make_public()
                video_url = blob.public_url
            except:
                video_url = f"https://storage.googleapis.com/{gcs_client.bucket_name}/{video_gcs_path}"
                
            return {
                "status": "success",
                "video_url": video_url,
                "script": script,
                "document_id": document_id
            }
            
    except Exception as e:
        logger.error(f"[VIDEO] Error generating video briefing: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/payroll")
async def get_payroll():
    """Get payroll data - stub endpoint"""
    logger.info("[PAYROLL] Get payroll request received")
    return {
        "payroll": [],
        "total": 0,
        "message": "Payroll data is not available. This endpoint is not implemented for RegLex compliance system."
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
